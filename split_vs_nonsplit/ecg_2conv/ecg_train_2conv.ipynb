{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDQY02P14s5b"
   },
   "source": [
    "# 1D-CNN Model for ECG Classification\n",
    "- The model used has 2 Conv. layers and 2 FC layers.\n",
    "- This code repeat running the training process and produce all kinds of data which can be given, such as data needed for drawing loss and accuracy graph through epochs, and maximum test accuracy for each run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jVwGea7meY-"
   },
   "source": [
    "## Get permission of Google Drive access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1384,
     "status": "ok",
     "timestamp": 1573140331176,
     "user": {
      "displayName": "Kyuyeon Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDhZm6xUbCYNLGqOYDpm2QRhOirby0BI-zUCTw2g=s64",
      "userId": "05245697685138289125"
     },
     "user_tz": -540
    },
    "id": "gw8VHp_Lqnu0",
    "outputId": "25ea812d-4242-463a-dc32-a938504f0792"
   },
   "outputs": [],
   "source": [
    "root_path = '../../'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-OoNrzR2jnb"
   },
   "source": [
    "## File name settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNGT9J0J2jEB"
   },
   "outputs": [],
   "source": [
    "data_dir = 'mitdb'\n",
    "train_name = 'train_ecg.hdf5'\n",
    "test_name = 'test_ecg.hdf5'\n",
    "all_name = 'all_ecg.hdf5'\n",
    "\n",
    "model_dir = 'model'\n",
    "model_name = 'conv2'\n",
    "model_ext = '.pth'\n",
    "\n",
    "csv_dir = 'csv'\n",
    "csv_ext = '.csv'\n",
    "\n",
    "csv_name = 'conv2'\n",
    "csv_accs_name = 'accs_conv2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQhRMQcxmiiU"
   },
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ekg3kh1Tq1B_"
   },
   "outputs": [],
   "source": [
    "mode = 'CIFAR10'\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzsvXmVd0ayn"
   },
   "source": [
    "## GPU settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1561,
     "status": "ok",
     "timestamp": 1573140331478,
     "user": {
      "displayName": "Kyuyeon Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDhZm6xUbCYNLGqOYDpm2QRhOirby0BI-zUCTw2g=s64",
      "userId": "05245697685138289125"
     },
     "user_tz": -540
    },
    "id": "1UJyXe89cJcw",
    "outputId": "14b01434-7392-4af6-98a1-f48524453a56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-DnZ3zF6dhx"
   },
   "source": [
    "## Define `ECG` `Dataset` class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffSLi6qCbHy2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loaded CIFAR10 datasets with batch size 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ECG(Dataset):\n",
    "    def __init__(self, mode='train'):\n",
    "        if mode == 'train':\n",
    "            with h5py.File(os.path.join(root_path, data_dir, train_name), 'r') as hdf:\n",
    "                self.x = hdf['x_train'][:]\n",
    "                self.y = hdf['y_train'][:]\n",
    "        elif mode == 'test':\n",
    "            with h5py.File(os.path.join(root_path, data_dir, test_name), 'r') as hdf:\n",
    "                self.x = hdf['x_test'][:]\n",
    "                self.y = hdf['y_test'][:]\n",
    "        elif mode == 'all':\n",
    "            with h5py.File(os.path.join(root_path, data_dir, all_name), 'r') as hdf:\n",
    "                self.x = hdf['x'][:]\n",
    "                self.y = hdf['y'][:]\n",
    "        else:\n",
    "            raise ValueError('Argument of mode should be train, test, or all.')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx], dtype=torch.float), torch.tensor(self.y[idx])\n",
    "\n",
    "if mode == 'ECG':\n",
    "    batch_size = 32\n",
    "    train_dataset = ECG(mode='train')\n",
    "    test_dataset = ECG(mode='test')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    x_train, y_train = next(iter(train_loader))\n",
    "    print(x_train.size())\n",
    "    print(y_train.size())\n",
    "    total_batch = len(train_loader)\n",
    "    print(total_batch)\n",
    "elif mode == 'CIFAR10':\n",
    "    batch_size = 32\n",
    "\n",
    "    root_path = '../../data/'\n",
    "    batch_size = 32\n",
    "\n",
    "    cifar10_mean = (0.485, 0.456, 0.406)\n",
    "    cifar10_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "    ])\n",
    "    train_dataset = CIFAR10(root=root_path, train=True, download=True, transform=transform)\n",
    "    test_dataset = CIFAR10(root=root_path, train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"Loaded CIFAR10 datasets with batch size\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2LRE1T2ZZLB"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, vgg16\n",
    "\n",
    "if mode == 'ECG':\n",
    "    class ECGConv(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ECGConv, self).__init__()\n",
    "            self.conv1 = nn.Conv1d(1, 16, 7, padding=3)  # 128 x 16\n",
    "            self.relu1 = nn.ReLU()\n",
    "            self.pool1 = nn.MaxPool1d(2)  # 64 x 16\n",
    "            self.conv2 = nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
    "            self.relu2 = nn.ReLU()\n",
    "            self.pool2 = nn.MaxPool1d(2)  # 32 x 16\n",
    "            self.linear3 = nn.Linear(32 * 16, 128)\n",
    "            self.relu3 = nn.ReLU()\n",
    "            self.linear4 = nn.Linear(128, 5)\n",
    "            self.softmax4 = nn.Softmax(dim=1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = self.relu1(x)\n",
    "            x = self.pool1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.relu2(x)\n",
    "            x = self.pool2(x)\n",
    "            x = x.view(-1, 32 * 16)\n",
    "            x = self.linear3(x)\n",
    "            x = self.relu3(x)\n",
    "            x = self.linear4(x)\n",
    "            x = self.softmax4(x)\n",
    "            return x\n",
    "\n",
    "elif mode == 'CIFAR10':\n",
    "    NUM_CLASSES = 10\n",
    "\n",
    "    class CIFAR10VGG(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CIFAR10VGG, self).__init__()\n",
    "            # Load pre-trained VGG16 model\n",
    "            self.model = vgg16(pretrained=True)\n",
    "\n",
    "            # Replace the classifier's last layer to match CIFAR10 classes (10 classes)\n",
    "            num_features = self.model.classifier[6].in_features\n",
    "            self.model.classifier[6] = nn.Linear(num_features, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "    \n",
    "    class AlexNet(nn.Module):\n",
    "        def __init__(self, num_classes=NUM_CLASSES):\n",
    "            super(AlexNet, self).__init__()\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(256 * 2 * 2, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(4096, num_classes),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), 256 * 2 * 2)\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "        \n",
    "    # class ECGConv(nn.Module):\n",
    "    #     def __init__(self):\n",
    "    #         super(ECGConv, self).__init__()\n",
    "    #         self.conv1 = nn.Conv2d(3, 16, 7, padding=3)\n",
    "    #         self.relu1 = nn.LeakyReLU()\n",
    "    #         self.pool1 = nn.MaxPool2d(2)\n",
    "    #         self.conv2 = nn.Conv2d(16, 16, 5, padding=2)\n",
    "    #         self.relu2 = nn.LeakyReLU()\n",
    "    #         self.pool2 = nn.MaxPool2d(2)\n",
    "    #         self.linear3 = nn.Linear(8 * 8 * 16, 128)\n",
    "    #         self.relu3 = nn.LeakyReLU()\n",
    "    #         self.linear4 = nn.Linear(128, 10)\n",
    "    #         self.softmax4 = nn.Softmax(dim=1)\n",
    "        \n",
    "    #     def forward(self, x):\n",
    "    #         x = self.conv1(x)\n",
    "    #         x = self.relu1(x)\n",
    "    #         x = self.pool1(x)\n",
    "    #         x = self.conv2(x)\n",
    "    #         x = self.relu2(x)\n",
    "    #         x = self.pool2(x)\n",
    "    #         x = x.view(-1, 8 * 8 * 16)\n",
    "    #         x = self.linear3(x)\n",
    "    #         x = self.relu3(x)\n",
    "    #         x = self.linear4(x)\n",
    "    #         x = self.softmax4(x)\n",
    "    #         return x\n",
    "\n",
    "    # class ECGConv(nn.Module):\n",
    "    #     def __init__(self):\n",
    "    #         super(ECGConv, self).__init__()\n",
    "    #         # Load pre-trained ResNet18 model\n",
    "    #         self.model = resnet18()\n",
    "\n",
    "    #         num_ftrs = self.model.fc.in_features\n",
    "    #         self.model.fc = nn.Linear(num_ftrs, 10)  # Replace num_output_classes with your number of classes\n",
    "\n",
    "    #     def forward(self, x):\n",
    "    #         return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-QMUMFx2n5J"
   },
   "source": [
    "## Training process settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3AKAftHyzPfz"
   },
   "outputs": [],
   "source": [
    "run = 1\n",
    "epoch = 400\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-bCqOUuxSdp"
   },
   "source": [
    "## Traning function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nO67O2Kj7B8J"
   },
   "outputs": [],
   "source": [
    "def train(nrun, model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = list()\n",
    "    train_accs = list()\n",
    "\n",
    "    test_losses = list()\n",
    "    test_accs = list()\n",
    "\n",
    "    for e in range(epoch):\n",
    "        print(\"Epoch {} - \".format(e+1), end='')\n",
    "\n",
    "        # train\n",
    "        train_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        for _, batch in enumerate(train_loader):\n",
    "            x, label = batch  # get feature and label from a batch\n",
    "            x, label = x.to(device), label.to(device)  # send to device\n",
    "            optimizer.zero_grad()  # init all grads to zero\n",
    "            output = model(x)  # forward propagation\n",
    "            loss = criterion(output, label)  # calculate loss\n",
    "            loss.backward()  # backward propagation\n",
    "            optimizer.step()  # weight update\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            correct += torch.sum(output.argmax(dim=1) == label).item()\n",
    "            total += len(label)\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accs.append(correct / total)\n",
    "        print(\"loss: {:.4f}, acc: {:.2f}%\".format(train_losses[-1], train_accs[-1]*100), end=' / ')\n",
    "        \n",
    "        # test\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            correct, total = 0, 0\n",
    "            for _, batch in enumerate(test_loader):\n",
    "                x, label = batch\n",
    "                x, label = x.to(device), label.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output, label)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                correct += torch.sum(output.argmax(dim=1) == label).item()\n",
    "                total += len(label)\n",
    "            test_losses.append(test_loss / len(test_loader))\n",
    "            test_accs.append(correct / total)\n",
    "        print(\"test_loss: {:.4f}, test_acc: {:.2f}%\".format(test_losses[-1], test_accs[-1]*100))\n",
    "\n",
    "    return train_losses, train_accs, test_losses, test_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5h1yJeZXRWjT"
   },
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AXFHuejuRe2u"
   },
   "source": [
    "### Repeat for 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6513611,
     "status": "ok",
     "timestamp": 1573150871777,
     "user": {
      "displayName": "Kyuyeon Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDhZm6xUbCYNLGqOYDpm2QRhOirby0BI-zUCTw2g=s64",
      "userId": "05245697685138289125"
     },
     "user_tz": -540
    },
    "id": "IAusksoSKSI0",
    "outputId": "3bf51e50-8a79-445c-f42e-d446d684603e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Epoch 1 - loss: 1.6942, acc: 35.12% / test_loss: 1.4396, test_acc: 46.39%\n",
      "Epoch 2 - loss: 1.3385, acc: 51.79% / test_loss: 1.2352, test_acc: 56.07%\n",
      "Epoch 3 - loss: 1.1839, acc: 58.13% / test_loss: 1.1567, test_acc: 60.08%\n",
      "Epoch 4 - loss: 1.0857, acc: 62.14% / test_loss: 1.1251, test_acc: 61.69%\n",
      "Epoch 5 - loss: 1.0030, acc: 65.16% / test_loss: 1.1095, test_acc: 62.57%\n",
      "Epoch 6 - loss: 0.9434, acc: 67.49% / test_loss: 1.1073, test_acc: 63.55%\n",
      "Epoch 7 - loss: 0.8945, acc: 69.27% / test_loss: 1.0815, test_acc: 65.63%\n",
      "Epoch 8 - loss: 0.8495, acc: 70.68% / test_loss: 1.1307, test_acc: 64.33%\n",
      "Epoch 9 - loss: 0.8036, acc: 72.21% / test_loss: 1.1326, test_acc: 63.43%\n",
      "Epoch 10 - loss: 0.7810, acc: 73.25% / test_loss: 1.1247, test_acc: 65.12%\n",
      "Epoch 11 - loss: 0.7624, acc: 73.78% / test_loss: 1.1534, test_acc: 66.07%\n",
      "Epoch 12 - "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m ecgnet \u001b[39m=\u001b[39m AlexNet()\n\u001b[0;32m     14\u001b[0m torch\u001b[39m.\u001b[39msave(ecgnet\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39minit_weight\u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m train_losses, train_accs, test_losses, test_accs \u001b[39m=\u001b[39m train(i, ecgnet\u001b[39m.\u001b[39;49mto(device))\n\u001b[0;32m     17\u001b[0m best_test_accs\u001b[39m.\u001b[39mappend(\u001b[39mmax\u001b[39m(test_accs))\n\u001b[0;32m     18\u001b[0m best_test_acc_epoch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(test_accs)\u001b[39m.\u001b[39margmax() \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(nrun, model)\u001b[0m\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# weight update\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> 27\u001b[0m     correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(output\u001b[39m.\u001b[39;49margmax(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m==\u001b[39m label)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     28\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(label)\n\u001b[0;32m     29\u001b[0m train_losses\u001b[39m.\u001b[39mappend(train_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_test_accs = list()\n",
    "\n",
    "for i in range(run):\n",
    "    print('Run', i+1)\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    ecgnet = AlexNet()\n",
    "    torch.save(ecgnet.state_dict(), f'init_weight{mode}.pth')\n",
    "    train_losses, train_accs, test_losses, test_accs = train(i, ecgnet.to(device))\n",
    "\n",
    "    best_test_accs.append(max(test_accs))\n",
    "    best_test_acc_epoch = np.array(test_accs).argmax() + 1\n",
    "    print('Best test accuracy {:.2f}% in epoch {}.'.format(best_test_accs[-1]*100, best_test_acc_epoch))\n",
    "    print('-' * 100)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'loss': train_losses,\n",
    "        'test_loss': test_losses,\n",
    "        'acc': train_accs,\n",
    "        'test_acc': test_accs\n",
    "    })\n",
    "    df.to_csv(os.path.join(root_path, csv_dir, '_'.join([csv_name, str(i+1)]) + csv_ext))\n",
    "\n",
    "df = pd.DataFrame({'best_test_acc': best_test_accs})\n",
    "df.to_csv(os.path.join(root_path, csv_dir, csv_accs_name + csv_ext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cpDwI7BFRjK6"
   },
   "source": [
    "## Print the best test accuracy of each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 141,
     "status": "ok",
     "timestamp": 1573150872213,
     "user": {
      "displayName": "Kyuyeon Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDhZm6xUbCYNLGqOYDpm2QRhOirby0BI-zUCTw2g=s64",
      "userId": "05245697685138289125"
     },
     "user_tz": -540
    },
    "id": "Ne8dr7DaRUME",
    "outputId": "9a98648b-ee04-490e-de87-4c3e1095b2e7"
   },
   "outputs": [],
   "source": [
    "for i, a in enumerate(best_test_accs):\n",
    "    print('Run {}: {:.2f}%'.format(i+1, a*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'csv\\\\conv2_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m'\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mconv2_1.csv\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      2\u001b[0m test_accs \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtest_acc\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m train_accs \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'csv\\\\conv2_1.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join('csv', 'conv2_1.csv'))\n",
    "test_accs = df['test_acc']\n",
    "train_accs = df['acc']\n",
    "test_losses = df['test_loss']\n",
    "train_losses = df['loss']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "ax[0].plot(train_losses)\n",
    "ax[0].plot(test_losses)\n",
    "ax[0].set_xticks([0, 100, 200, 300, 400])\n",
    "ax[0].set_xlabel('Epoch', size=16)\n",
    "ax[0].set_ylabel('Loss', size=16)\n",
    "ax[0].set_ylim(0.9, 1.1)\n",
    "ax[0].set_yticks([0.9, 1.0, 1.1, 1.2])\n",
    "ax[0].grid(alpha=0.5)\n",
    "ax[0].tick_params(labelsize=16)\n",
    "ax[0].legend(['Train', 'Test'], loc='right', fontsize=16)\n",
    "\n",
    "ax[1].set_ylim(0.7, 1.0)\n",
    "ax[1].set_yticks([0.7, 0.8, 0.9, 1.0])\n",
    "ax[1].plot(train_accs)\n",
    "ax[1].plot(test_accs)\n",
    "yt = ax[1].get_yticks()\n",
    "ax[1].set_yticklabels(['{:,.0%}'.format(x) for x in yt])\n",
    "ax[1].set_xticks([0, 100, 200, 300, 400])\n",
    "ax[1].set_xlabel('Epoch', size=16)\n",
    "ax[1].set_ylabel('Accuracy', size=16, labelpad=-5)\n",
    "ax[1].grid(alpha=0.5)\n",
    "ax[1].tick_params(labelsize=16)\n",
    "ax[1].legend(['Train', 'Test'], loc='right', fontsize=16)\n",
    "\n",
    "fig.savefig('loss_acc_conv2_seed.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ecg_train_2conv_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
